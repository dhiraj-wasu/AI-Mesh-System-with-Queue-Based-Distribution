AI Mesh: Distributed Task Processing with FastAPI + Redis

A fault-tolerant, auto-scaling, AI-powered task orchestration system â€” built with FastAPI, Redis, Docker, and AI workers.
Handles text and image tasks with retries, timeouts, and dynamic scaling, inspired by production-grade MLOps pipelines.

Features

1.Redis-backed Queues for reliable task distribution (text_queue, image_queue)

2.Multi-worker Architecture (text & image workers run independently)

3.Fault Tolerance with retries (3x), timeouts, and dead-letter queue

4.Worker Health Monitoring via heartbeat + Redis TTL

5.Auto-scaling with load-aware worker scaling (Docker Compose/K8s)

6.Task Complexity Tagging (small vs large) for smarter scaling

7.Result Tracking with Redis hashes (status, retries, timestamps, result)

8.Dockerized for easy deployment & scaling across environments

Tech Stack

FastAPI â†’ REST API for task submission & status

Redis â†’ Queue + task store + worker monitoring

Docker Compose â†’ Spin up API + Redis + workers

Transformers (Hugging Face) â†’ Text sentiment analysis

OpenCV â†’ Image face detection

Python Threading â†’ Concurrent workers per container

ğŸ“‚ Project Structure
ai-mesh/
â”‚â”€â”€ app/
â”‚   â”œâ”€â”€ main.py           # FastAPI API endpoints
â”‚   â”œâ”€â”€ redis_queue.py    # Redis connection
â”‚   â”œâ”€â”€ enqueue.py        # Task enqueue logic + complexity tagging
â”‚â”€â”€ workers/
â”‚   â”œâ”€â”€ text_worker.py    # Processes text tasks
â”‚   â”œâ”€â”€ image_worker.py   # Processes image tasks
â”‚â”€â”€ scaler/
â”‚   â”œâ”€â”€ auto_scale.py     # Dynamic worker scaling logic
â”‚â”€â”€ docker-compose.yml    # Multi-container setup
â”‚â”€â”€ Dockerfile            # Base image
â”‚â”€â”€ README.md             # This file ğŸš€

Getting Started
1. Clone the repo
git clone https://github.com/yourusername/ai-mesh.git
cd ai-mesh

2. Start with Docker Compose
docker-compose up --build


This starts:

FastAPI API (on port 8000)

Redis (as queue broker)

Workers (text + image processors)

3. Submit a Task
curl -X POST "http://localhost:8000/submit" \
     -H "Content-Type: application/json" \
     -d '{"type": "text", "data": "I love this project!"}'


Response:

{"task_id": "123e4567-e89b-12d3-a456-426614174000"}

4. Check Task Status
curl "http://localhost:8000/status/123e4567-e89b-12d3-a456-426614174000"


Example response:

{
  "status": "done",
  "result": {"label": "POSITIVE", "score": 0.99}
}

Auto-Scaling Logic

Every 10s, scaler checks queue length & task complexity

Dynamically scales workers between 1 â†’ 8

Large tasks weigh more â†’ trigger faster scaling

Idle workers are scaled down after 3 empty cycles

Example Workflows

âœ… Text Analysis â†’ Sentiment analysis using Hugging Face Transformers
âœ… Image Processing â†’ Face detection using OpenCV
âœ… Fault Tolerance â†’ Retries up to 3x, then sent to dead-letter queue
âœ… Auto-Scaling â†’ Spin up more workers if queues get heavy

 Roadmap

 Add Prometheus + Grafana monitoring

 Support priority queues (high, medium, low)

 Add more AI models (summarization, OCR, speech-to-text)

 Implement DAG task dependencies

 Deploy on Kubernetes with HPA

Use Cases

AI Inference Gateway â€“ Centralized API for ML models

ğŸ–¼ï¸ Batch Media Processing â€“ Scale image/video processing pipelines

ğŸ“„ NLP Workflows â€“ Text classification, sentiment, summarization

ğŸ§‘â€ğŸ¤â€ğŸ§‘ Multi-tenant SaaS â€“ Queue isolation per client with quotas

ğŸ† Why "AI Mesh"?

Because itâ€™s like a mesh of AI workers dynamically scaling, self-healing, and collaborating to process tasks in real-time â€” just like production AI systems.
