

# **AI Mesh: Distributed Task Processing with FastAPI + Redis**

A fault-tolerant, auto-scaling, AI-powered task orchestration system built with **FastAPI**, **Redis**, **Docker**, and AI workers.

Inspired by production-grade **MLOps pipelines**, this system handles text and image tasks with retries, timeouts, worker health monitoring, and dynamic scaling.

---

## ğŸš€ Features

* **Redis-backed Queues** for reliable task distribution (`text_queue`, `image_queue`)
* **Multi-worker Architecture** (text & image workers run independently)
* **Fault Tolerance** with retries (3Ã—), timeouts, and dead-letter queue
* **Worker Health Monitoring** via heartbeat + Redis TTL
* **Auto-scaling** with load-aware worker scaling
* **Task Complexity Tagging** (small vs large) for smarter scaling
* **Result Tracking** with Redis hashes (status, retries, timestamps, result)
* **Dockerized Deployment** for easy scaling across environments

---

## ğŸ§± Tech Stack

* **FastAPI** â†’ REST API for task submission & status
* **Redis** â†’ Queue broker + task store + worker monitoring
* **Docker Compose** â†’ Multi-container orchestration
* **Transformers (Hugging Face)** â†’ Text sentiment analysis
* **OpenCV** â†’ Image face detection
* **Python Threading** â†’ Concurrent workers per container

---

## ğŸ“‚ Project Structure

```
ai-mesh/
â”‚â”€â”€ app/
â”‚   â”œâ”€â”€ main.py          # FastAPI API endpoints
â”‚   â”œâ”€â”€ redis_queue.py   # Redis connection
â”‚   â”œâ”€â”€ enqueue.py       # Task enqueue logic + complexity tagging
â”‚
â”‚â”€â”€ workers/
â”‚   â”œâ”€â”€ text_worker.py   # Processes text tasks
â”‚   â”œâ”€â”€ image_worker.py  # Processes image tasks
â”‚
â”‚â”€â”€ scaler/
â”‚   â”œâ”€â”€ auto_scale.py    # Dynamic worker scaling logic
â”‚
â”‚â”€â”€ docker-compose.yml   # Multi-container setup
â”‚â”€â”€ Dockerfile           # Base image
â”‚â”€â”€ README.md            # Documentation
```

---

## âš¡ Getting Started

### Clone the Repository

```bash
git clone https://github.com/yourusername/ai-mesh.git
cd ai-mesh
```

### Start the System

```bash
docker-compose up --build
```

This starts:

* FastAPI API (port **8000**)
* Redis (queue broker)
* Workers (text + image processors)

---

## ğŸ“¬ Submit a Task

```bash
curl -X POST "http://localhost:8000/submit" \
-H "Content-Type: application/json" \
-d '{"type": "text", "data": "I love this project!"}'
```

**Response**

```json
{"task_id": "123e4567-e89b-12d3-a456-426614174000"}
```

---

## ğŸ” Check Task Status

```bash
curl "http://localhost:8000/status/123e4567-e89b-12d3-a456-426614174000"
```

**Example Response**

```json
{
  "status": "done",
  "result": {"label": "POSITIVE", "score": 0.99}
}
```

---

## ğŸ“ˆ Auto-Scaling Logic

Every **10 seconds**, the scaler:

* Checks queue length & task complexity
* Dynamically scales workers between **1 â†’ 8**
* Large tasks weigh more â†’ trigger faster scaling
* Idle workers scale down after **3 empty cycles**

---

## âœ… Example Workflows

* **Text Analysis** â†’ Sentiment analysis using Hugging Face Transformers
* **Image Processing** â†’ Face detection using OpenCV
* **Fault Tolerance** â†’ Retries up to 3Ã— â†’ Dead-letter queue fallback
* **Auto-Scaling** â†’ Workers scale based on system load

---

## ğŸ›£ï¸ Roadmap

* Add Prometheus + Grafana monitoring
* Support priority queues (high / medium / low)
* Add more AI models (summarization, OCR, speech-to-text)
* Implement DAG task dependencies
* Deploy on Kubernetes with HPA

---

## ğŸ’¡ Use Cases

* **AI Inference Gateway** â€“ Centralized API for ML models
* **Batch Media Processing** â€“ Scalable image/video pipelines
* **NLP Workflows** â€“ Classification, sentiment, summarization
* **Multi-tenant SaaS** â€“ Queue isolation & quotas

---

## ğŸ† Why *"AI Mesh"*?

Because it behaves like a mesh of AI workers â€” dynamically scaling, self-healing, and collaboratively processing tasks in real time â€” similar to real-world production AI systems.

---

